---
title: "004 - How to use the PSA DB"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
    toc_float: yes
    code_folding: none
    includes:
      before_body: assets/header.html
      after_body: assets/footer.html
---

```{r echo = FALSE}
options(crayon.enabled = FALSE)
```

## Links to other notebooks

1. [What is a DB?](001_What_is_a_database.nb.html)
2. [Getting started with R](002_Getting_started_with_R.nb.html)
3. [How to use a local DB](003_How_to_use_a_local_DB.nb.html)
4. [How to use the PSA DB](004_How_to_use_the_PSA_DB.nb.html)

# Getting credentials

To follow along with this tutorial (and eventually use the database for real analyses) you need to get the login credentials. Rohit is the contact person for that, and you can see how I've stored them below. I have a fake file called `secret-example.R` that's in this repository, so you can copy it and paste in your own username and password (without the **`< >`**).

Source file: [`secret-example.R`](secret-example.R)

```{r echo=FALSE}
cat(readLines("secret-example.R"), sep = "\n")
```

Then save it as `secret.R` and this code chunk will run it, storing those 5 variables in the global environment.

```{r}
source("secret.R")
```

# Use the credentials to connect

Now we can use them to connect to the database:

```{r echo = FALSE}
# Clean up an old connection before adding a new one

if (exists("con") && 
    class(con) == "PqConnection" && 
    DBI::dbIsValid(con) &&
    DBI::dbGetInfo(con)$dbname == "crowndb") {
  DBI::dbDisconnect(con)
  invisible()
}
```

```{r}
library(dplyr, warn.conflicts = FALSE)
library(dbplyr, warn.conflicts = FALSE)
library(RPostgres)

con <- dbConnect(
  Postgres(),
  dbname = pg_dbname,
  host = pg_host,
  port = pg_port,
  user = pg_user,
  password = pg_password,
  sslmode = "require",
  bigint = "numeric"
)
```

> **NOTE:** Remember to close connections with **`dbDisconnect(con)`** at the end of your R script, but occasionally your connection will be reset without you doing it explicitly. That could be that the DB thought you were inactive, or a number of other reasons (like your WiFi is spotty). If you find that queries time out or are not responding, just reconnect using this chunk of code.

> **WARNING:** Pay attention to the returned type of numeric values. Computers nowadays use 64-bit processors, which means they're able to represent very large numbers in memory. However, not all programs implement them in the same way. 
>
> When we made the connection above, we specified that the Postgres data type **`BIGINT`** should be returned as regular R **`numeric`** (a.k.a. a **`double`**-precision floating point number, or 64 bits). With the scale of data we have, that won't be a problem, as **`double`**s can hold data up to `1.797693e+308` (which you can find with `.Machine$double.xmax`). If you need exact integers, you can use **`bigint = "integer"`** when you make your database connection, but that only supports values up to `2147483647` (since they're signed 32-bit integers). 
>
> I don't recommend using the default, which is `bigint = "integer64"`. This is a new class of 64-bit integers which doesn't have full support yet, so arithmetic and subsetting can have unexpected results.

## Explore the DB

First thing, let's make sure this is the right database and everything looks correct. We should see what tables are available:

```{r}
dbListTables(con)
```

Let's look at one of the tables, the list of cover crop species:

```{r paged.print = FALSE}
dbReadTable(con, "cc_species")
```

Notice that it returned the whole table, since this was not a lazy query. For a small table like this, that's not a problem. But some of the tables are **many** rows. If you try to pull them all in using **`dbReadTable`**, you'll end up waiting a while for some tables.

```{r paged.print = FALSE}
# Takes ~11s to return
dbListTables(con) %>% 
  stringr::str_subset("^pg", negate = TRUE) %>% 
  purrr::set_names() %>% 
  purrr::map_dfr(
    ~tbl(con, .x) %>% 
      tally() %>% 
      collect(),
    .id = "table"
    ) %>%
  print(n = Inf)
```

That's a helpful list, but that was also a mouthful of code. Let's break it down a bit. The following is identical code, but written with intermediate variables instead of chaining with the **`%>%`** (*pipe*) operator. If you're familiar with base R instead, you could use **`lapply`** instead of **`purrr::map_*`**, but `{purrr}` handles a lot of details like output type for you. You don't need to know `{purrr}` for the rest of this tutorial, but if you write a lot of loops, it will improve your code. The output isn't shown below, since it's the same as above.

```{r eval = FALSE, paged.print = FALSE}
# get the list of tables as a vector
tables <- dbListTables(con)
tables

# discard the tables that begin with "pg"
clean_tables <- stringr::str_subset(tables, "^pg", negate = TRUE)
clean_tables

# name the vector so we can identify each row later
clean_tables <- purrr::set_names(clean_tables)
clean_tables

# helper function to count rows as a lazy query
nrows_remote <- function(name) {
  
  remote_table <- tbl(con, name)       # lazily pull the table
  
  remote_count <- tally(remote_table)  # count the rows on the DB side, 
                                       #   not locally
  
  collect(remote_count)                # pull the count in to your local 
                                       #   R environment
}

# loop over each table and apply the helper function, 
#   row-binding the results to a single tibble/dataframe
purrr::map_dfr(
  clean_tables
  ~nrows_remote(.x),
  .id = "table"
  )

# `print(..., n=Inf)` just makes sure it prints 
#   all the rows to the output
```

# Make some simple queries

Okay, now that we're set up and we know some pitfalls to watch out for, let's try a query to look at the sites enrolled. Remember, since it's a lazy query, we don't know how many rows there are until we **`collect()`** it.

```{r paged.print = FALSE}
tbl(con, "site_information")
```

Remember that this is identical to previewing `SELECT * FROM site_information` in vanilla SQL. Let's see how many sites were enrolled in each year:

```{r paged.print = FALSE}
year_data <- tbl(con, "site_information") %>% 
  arrange(year) %>% 
  group_by(year) %>% 
  tally()

year_data

show_query(year_data)
```

What about if we wanted to store a list of the site codes and what state and year they were enrolled in?

```{r paged.print = FALSE}
site_list <- tbl(con, "site_information") %>% 
  select(year, state, code, latitude, longitude) %>% 
  arrange(year, state, code) %>% 
  collect()

site_list
```

Now you could do any local computation or manipulation you wanted. My rule of thumb is that if it's one of the common operations (**`select`**ing columns, **`arrange`**ing to sort, **`filter`**ing rows, **`*_join`**ing two tables), go ahead and do those with lazy queries. If you have more complex operations you want to do, it might be better to pull the table in locally with **`collect()`**. Again, it's a good idea to check how many rows will be returned before you do that though:

```{r paged.print = FALSE}
tbl(con, "site_information") %>% 
  select(year, state, code, latitude, longitude) %>% 
  arrange(year, state, code) %>% 
  tally()
```

## Mapping the sites

So let's do some local computation on the **`site_list`** object we stored above. First we'll make a static map using the `{ggplot2}` package, which is good for making high quality customizable images. Then we'll make an interactive map with `{leaflet}`, a JavaScript package that embeds maps in documents like these or web applications. 

We won't fuss around too much with the options in either, but both are powerful enough to do any kind of mapping visualization you can imagine. You don't need any of the other packages I'm using here if you aren't making maps, but they're handy to have if you are.

```{r paged.print = FALSE}
state_outlines <- 
  rnaturalearthdata::states50 %>% 
  sf::st_as_sf() %>% 
  filter(sr_adm0_a3 == "USA")

library(ggplot2)
ggplot() + 
  geom_sf(data = state_outlines) +
  geom_point(
    data = site_list, 
    aes(longitude, latitude, color = year)
    ) +
  coord_sf(
    xlim = range(site_list$longitude, na.rm = TRUE),
    ylim = range(site_list$latitude, na.rm = TRUE)
  )

library(leaflet)
leaflet(
  site_list,
  options = leafletOptions(maxZoom = 6)
  ) %>% 
  addTiles() %>% 
  addMarkers(
    ~round(longitude, 1),
    ~round(latitude, 1)
  )
```


## Reducing rows returned

With the mapping example we wanted all the sites, but some tables just have a lot of rows. You probably want to use a subset of those rows to start your model or visualize before jumping in with the whole thing.

We saw above that the largest table in the DB is the **`"weather"`** table. Let's look at just the first few rows with a lazy query:

```{r paged.print = FALSE}
tbl(con, "weather")
```

We learned in the last notebook on SQLite how to filter on text and numeric columns, but there's two new ones here, **`date`** and **`datetime`**. Really these are still both just numbers, usually counting something like the number of seconds since 1970-Jan-01, or something like that. But it's hard to think in terms of when `18262` is, or `1577854800`. You're much more familiar with working with **January 1, 2020** and **2020-01-01 12:00AM EST**.

There are two main ways to work with date(time)s in R. In base R, there's **`as.Date`** and **`as.POSIXct`** (but the documentation to use the formats is in **`?strptime`**) and in the `{tidyverse}`, there's a whole package called `{lubridate}`. If that seems too complicated, you're right. Dates and times [are notorious](https://xkcd.com/1179/){target="_blank"} for being a bear in every programming language and on every computer platform. It's easiest if you use an unambiguous international standard, like [ISO-8601](https://en.wikipedia.org/wiki/ISO_8601){taget="_blank"}, [further reading](https://www.cl.cam.ac.uk/~mgk25/iso-time.html){target="_blank"}.

```{r paged.print = FALSE}
dts <- c("1984-08-23", "2020-01-01")
tms <- c("1984-08-23 7:30:00 PM", "2020-01-01 12:00:00 AM")
iso <- c("1984-08-23 19:30:00",   "2020-01-01 00:00:00")

data.frame(
  dates = as.Date(dts),
  times = as.POSIXct(tms, format = "%Y-%m-%d %I:%M:%S %p"),
  iso = as.POSIXct(iso)
)

tibble(
  dates = lubridate::as_date(dts),
  times = lubridate::as_datetime(tms),
  iso = lubridate::as_datetime(iso)
)
```

Notice that using the standard format in your text input makes your code a lot simpler to read. Likewise, `{lubridate}` is pretty good at guessing, so it can be a handy shortcut as well. I'll stick with base R for this example though, but I generally prefer the other package.

So let's say instead of the million or so rows that **`"weather"`** has, we want just the observations from July 4, 2018. If we're using the **`date`** column, we would need to select exact equality. But if we're using the **`timez`** column, we want to select a range: everything greater than midnight that morning and less than 11:59:59PM that night.

```{r paged.print = FALSE}
date_query <- tbl(con, "weather") %>% 
  filter(date == as.Date("2018-07-04"))

date_query

show_query(date_query)

date_query %>% 
  group_by(code) %>% 
  tally()
```

So we can see that 24 hours of observations are returned in our query for each site. Let's try it with times instead:

```{r paged.print = FALSE}
t_start <- as.POSIXct("2018-07-04 00:00:00")
t_end <- as.POSIXct("2018-07-04 23:59:59")

time_query <- tbl(con, "weather") %>% 
  filter(between(timez, t_start, t_end))

time_query

show_query(time_query)

time_query %>% 
  group_by(code) %>% 
  tally()
```

We get **almost** the same results. Note that instead of **`dplyr::between`**, we could have said `filter(timez >= t_start, timez <= t_end)`, and it would have been the same. Sometimes I prefer the long-winded way of writing it, if it's clearer to read the code later.

> **WARNING:** You have to watch your timezones carefully. You'll notice in the translated SQL, it used `'2018-07-04T04:00:00Z'` as our start time. I am writing this code on a machine in the `America/New_York` time zone, and on July 4 2018, that was equivalent to a `-4` offset (because of Daylight Saving Time, it's `-5` the rest of the year). Thus R assumed that any time I put in without an explicit timezone is my local time, while the **`timez`** column in the DB is UTC (or "Zulu") time.

> **NOTE:** Whether the first query (the whole of 2018-07-04 at the prime meridian) or the second query (between midnight and midnight on 2018-07-04 on the east coast) is what you want is up to you. Just be aware of what times you're actually asking for and getting in return.


# Joining two tables

We already explored the most important table, **`"site_information"`**. It's important, because it keys the three letter farm codes to all the other data about each farm. What about **`"decomp_biomass"`**? That's all the data about the litterbag decomposition study, and it's where dry weight of the cover crop biomass is stored.

```{r paged.print = FALSE}
tbl(con, "decomp_biomass")
```

But it only has the farm codes, so we need to join it with the site information table. What kind of join is appropriate? In the decomp data, there are multiple rows per farm code (24 if no bags got destroyed), and farm code is the only key that connects the two tables. This is called a one-to-many correspondence.

```{r paged.print = FALSE}
tbl(con, "decomp_biomass") %>% 
  group_by(code) %>% 
  tally()

tbl(con, "site_information") %>% 
  group_by(code) %>% 
  tally()
```

## Join types

Assuming we join them with **`"site_information"`** as the "left" table and **`"decomp_biomass"`** as the "right" table:

* A **`left_join()`** would return 
  + all the columns in both tables
  + only ONE row per code
* A **`right_join`** would return 
  + all the columns in both tables
  + all the rows from the decomp table
    + including rows with **no match** in the site info table (`NA`s for lat/long etc)
* An **`inner_join()`** would return
  + all the columns in both tables
  + MATCHING rows from both tables 
    + (no `NA`s)
* An **`outer_join()`** would return
  + all the columns in both tables
  + ALL rows from both tables 
    + (`NA`s in lat/long & as `NA`s in dry weight)
* A **`semi_join`** would return
  + only columns from site info
  + one row per code in the site info table 
    + (the first match from decomp table, no `NA`s)
* An **`anti_join`** would return
  + only columns from site info
  + ONE row per code where there's NO MATCH in decomp data
  +   (useful for finding all the mismatches/`NA`s)

I almost always want a **`full_join`**, so I can make sure I catch any mismatches and investigate them. A **`right_join`** is also good for "one-to-many" correspondences, as well as an **`inner_join`**. Beware of the dropped observations with inner joins; however, those rows would be "unidentifiable" because of missing data, so you'd end up dropping them in your analysis anyway. It's just something to keep in mind.

```{r paged.print = FALSE}
colnames(tbl(con, "site_information"))
colnames(tbl(con, "decomp_biomass"))
```

> **WARNING:** Always look at what columns joins are matching on if you're not specifying them manually. For example, both of these tables share the **`code`** column (which we want) as well as the **`cid`** column (which is an internal DB identifier). If we let it join on the **`cid`** column, we're gonna have trouble, since they won't ever match.

```{r paged.print = FALSE}
inner_join(
  tbl(con, "site_information"),
  tbl(con, "decomp_biomass"),
  by = "code"       # force using only the `code` col. for matching
)
```

Notice that now there's **`cid.x`** and **`cid.y`**, which are from the left and right tables respectively and don't match each other. This query returns way too many columns, so let's simplify the data so it's easier to look at:

```{r paged.print = FALSE}
bag_data <- inner_join(
  tbl(con, "site_information"),
  tbl(con, "decomp_biomass"),
  by = "code"
) %>% 
  select(
    code, year, state,          # vars to identify the site
    subplot, subsample, time,   # vars to identify the bag
    dry_biomass_wt, percent_n   # actual data we want
    )

show_query(bag_data)

bag_data
```

That nasty SQL statement could be simplified to:

```{r eval = FALSE, paged.print = FALSE}
dbGetQuery(con, 
  "
  SELECT LHS.code, year, state, time, dry_biomass_wt, percent_n
    FROM site_information AS LHS
    INNER JOIN decomp_biomass AS RHS
    ON (LHS.code = RHS.code)
  ")
```

But again, that would return the whole query at once, instead of lazily, so if you want to iterate over different versions of your query for testing, it can be slow and eat up memory. All the explicit naming and quoting that `{dbplyr}` is doing when it translates to SQL prevents a lot of errors, but if your hand-written SQL is better than mine, you can use it straight instead.

Let's look at rows that have already had the bags run on the C:N analyzer to get %N content:

```{r paged.print = FALSE}
n_data <- bag_data %>% 
  filter(!is.na(percent_n)) %>% 
  collect()

n_data

ggplot(n_data, aes(time, percent_n, group = time)) +
  geom_boxplot()
```

# More complex queries

Let's try an example with three tables. I'll split it up into several separate queries to make it a little more clear what each step is doing, but they could all be chained together into a single statement. 

1. We'll use the **`decomp_biomass`** table to estimate moisture content for bags at **`time == 0`** (first collection). 
2. Then we'll join it to the **`in_field_biomass`** table and calculate dry equivalents of the fresh biomass there. 
3. Then we'll join it back to the **`site_information`** table to get state and year information.
4. Then we'll convert the percent-quality columns into total $grams/meter^{2}$ units.
5. Finally we'll pull the query in locally and do some visualization and analysis.

## Moisture content estimates

```{r paged.print = FALSE}

bag_moisture_query <- tbl(con, "decomp_biomass") %>% 
  filter(time == 0) %>% 
  mutate(m_ratio = dry_biomass_wt / fresh_biomass_wt) %>% 
  group_by(code, subplot) %>% 
  summarize(m_ratio = mean(m_ratio, na.rm = TRUE))

bag_moisture_query

```

Here we looked up the decomp table, filtered to only the time-zero bags, and calculated the ratio of dry:fresh biomass weights. Then we separate it into groups for each farm code and subplot (should be two rows per group, subsamples A and B). For each group, we then calculate the mean of that moisture ratio.

## Get dry matter equivalents for bulk cover crop material

```{r paged.print = FALSE}

in_field_query <- full_join(
  bag_moisture_query,
  tbl(con, "in_field_biomass")
  ) %>% 
  mutate(
    dry_wt_est_g_m2 = m_ratio * (fresh_wt_a + fresh_wt_b) / 2
    )

in_field_query

```

The first query (**`bag_moisture_query`**) constructed a temporary table inside the database. Now we're telling the DB we want to do a full (or outer) join on that temporary table and the **`in_field_biomass`** table (containing all matches between rows, and every column in both tables). Since we constructed the temporary table to only have three columns (**`code`**, **`subplot`**, and **`m_ratio`**), we don't have to worry about specifying which columns to join on: we should get perfect 1:1 matching on **`code`** and **`subplot`**. A helpful message is printed letting us know those are the common columns.

After the join, we then make a new column in this new temporary table. The new column is the average of the two subsamples of fresh weights, times the moisture ratio we calculated before. Again, simple arithmetic operations like this are easy for the database to do internally. Now we have a temporary table with all the biomass properties, including an estimate of dry matter content in $grams/meter^{2}$.

## Combine with site info

```{r paged.print = FALSE}
biomass_with_site_query <- full_join(
  tbl(con, "site_information"),
  in_field_query,
  by = "code"
)

biomass_with_site_query
```

Now we take the temporary table and do another full/outer join, this time with the site info table. Since we didn't strip out any columns in the last query, it's a good idea to specify **`by = "code"`** to make sure it ignores any other common columns (in particular the internal row identifier, **`cid`**). Now we have a temporary table with all the biomass properties, as well as information about which state and year they were recorded in.

## Convert units

```{r paged.print = FALSE}
biomass_with_units_query <- biomass_with_site_query %>% 
  mutate_at(
    vars(matches("percent_")), 
    list("g_m2" = ~.*dry_wt_est_g_m2)
    )

# If you want to see a real mouthful, show the underlying SQL call
show_query(biomass_with_units_query)
```

The last step is to create new columns using an "anonymous function": **`~.*dry_wt_est_g_m2`**. This is the same thing as a function that looks like `multiplier <- function(x) { x * dry_wt_est_g_m2 }`. We then use that function on all the variables/columns (**`vars()`**) that match the text **`"percent_"`**. For every such column, it will now make a new column with the name **`percent_foo_g_m2`**, which will be the total amount of "foo" in $grams/meter^{2}$. 

You can also see I printed out the generated SQL query, since this is still all happening inside the database. You could have pulled the query in locally at a number of points during this chain, and the `{dplyr}` syntax would have been identical (the calls to **`filter`, `mutate`, `summarize`, `group_by`, `full_join`**); just the calls to **`tbl(con, "table_name")`** would have been your local variables instead.

Again, this is overly-explicit generated SQL, which could probably be simplified if you wrote it by hand. However, the `{dbplyr}`-translated version checks for a lot of possible errors and tries to optimize the number of operations for speed and memory-efficiency.

## Complete the query

```{r paged.print = FALSE}

local_biomass_df <- collect(biomass_with_units_query) %>% 
  filter(!is.na(state)) %>% 
  arrange(year, state)

local_biomass_df
```

Now all we had left was to pull that long 3-table query into your local R environment and do some visualization and analysis. 

## Analyzing the results

Let's look at hemicellulose content as an example.

```{r paged.print = FALSE}
local_biomass_df %>% 
  filter(!is.na(percent_hemicell_calc_g_m2)) %>% 
  ggplot(aes(state, percent_hemicell_calc_g_m2)) +
  geom_boxplot() + 
  facet_grid(
    ~year, 
    space = "free_x", 
    scales = "free_x"
    )
```

What about the relationships between the composition data? Let's look at crude protein content as a function of total dry matter:

```{r paged.print = FALSE}
local_biomass_df %>% 
  filter(!is.na(percent_cp), !is.na(dry_wt_est_g_m2)) %>%
  ggplot(aes(dry_wt_est_g_m2, percent_cp, color = factor(year))) +
  geom_point() +
  scale_x_log10() + 
  scale_y_log10() + 
  stat_smooth(method = "lm", se = FALSE)
```



# And don't forget to clean up after

```{r}
dbDisconnect(con)
```
